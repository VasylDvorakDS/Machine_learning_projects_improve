{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Методы фильтрации\n",
        "### Вычисление уменьшения энтропии"
      ],
      "metadata": {
        "id": "hfKlnA4iTGO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QctjnIPsS57p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "importances = mutual_info_classif(X, y)\n",
        "# Где data - ваш датасет; X, y – входные и выходные данные соответственно\n",
        "feature_importances = pd.Series(importances, data.columns[0:len(data.columns)-1])\n",
        "feature_importances.plot(kind='barh', color='teal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Критерий Хи"
      ],
      "metadata": {
        "id": "IUqTrG-WTQAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Преобразование в категориальные данные путем преобразования в целые числа.\n",
        "# Где X, y - входные и выходные данные соответственно.\n",
        "X_categorical = X.astype(int)\n",
        "\n",
        "# Выбираем 3 признака с наивысшим \"хи-квадрат\".\n",
        "chi2_features = SelectKBest(chi2, k = 3)\n",
        "X_kbest_features = chi2_features.fit_transform(X_categorical, y)\n",
        "\n",
        "# Вывод \"до и после\"\n",
        "print(\"Количество признаков до преобразования:\", X_categorical.shape[1])\n",
        "print(\"Количество признаков после преобразования:\", X_kbest_features.shape[1])"
      ],
      "metadata": {
        "id": "4UZ4vgTHTPa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F-тест"
      ],
      "metadata": {
        "id": "h8ID74TNTgd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from skfeature.function.similarity_based import fisher_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Вычисляем критерий\n",
        "# Где X, y - входные и выходные данные соответственно.\n",
        "ranks = fisher_score.fisher_score(X, y)\n",
        "\n",
        "# Делаем график наших \"фич\"\n",
        "# Где data - ваш датасет\n",
        "feature_importances = pd.Series(ranks, data.columns[0:len(data.columns)-1])\n",
        "feature_importances.plot(kind='barh', color='teal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VjAHUth_Taet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Корреляция"
      ],
      "metadata": {
        "id": "DDjjzo3iTr6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Матрица корреляции\n",
        "# Где data - ваш датасет\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Выводим признаки на тепловую карту\n",
        "plt.figure(figsize= (10, 6))\n",
        "sns.heatmap(correlation_matrix, annot = True)"
      ],
      "metadata": {
        "id": "291RDqFzTnUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Абсолютное отклонение"
      ],
      "metadata": {
        "id": "Cxe6cUv8T0Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "# Вычисляем MAD\n",
        "# Где X - входные данные\n",
        "mean_absolute_difference = np.sum(np.abs(X - np.mean(X, axis = 0)), axis = 0) / X.shape[0]\n",
        "\n",
        "# Наш график признаков\n",
        "plt.bar(np.arange(X.shape[1]), mean_absolute_difference, color = 'teal')"
      ],
      "metadata": {
        "id": "5-IsA81xTwox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Методы обертки\n",
        "### Прямой отбор признаков"
      ],
      "metadata": {
        "id": "dKgjkuWyUCed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', random_state=42, n_jobs=-1, max_iter=50e)\n",
        "ffs = SequentialFeatureSelector(lr, k_features='best', forward = True, n_jobs=-1)\n",
        "\n",
        "ffs.fit(X, Y)\n",
        "# X, y – входные и выходные данные соответственно.\n",
        "# X_train – входные данные с обучающейся выборки,\n",
        "# y_pred – выходные данные предиктора\n",
        "features = list(ffs.k_feature_names_)\n",
        "features = list(map(int, features))\n",
        "y_pred = lr.predict(X_train[features])"
      ],
      "metadata": {
        "id": "0dTS39zjUBd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Последовательный отбор признаков"
      ],
      "metadata": {
        "id": "woY-k6sfUb9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', random_state=42, n_jobs=-1, max_iter=50e)\n",
        "lr.fit(X, y)\n",
        "\n",
        "bfs = SequentialFeatureSelector(lr, k_features='best', forward = False, n_jobs=-1)\n",
        "bfs.fit(X, y)\n",
        "features = list(bfs.k_feature_names_)\n",
        "features = list(map(int, features))\n",
        "lr.fit(X_train[features], y_train)\n",
        "y_pred = lr.predict(x_train[features])"
      ],
      "metadata": {
        "id": "MKwtvuHzUcc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Исчерпывающий отбор признаков"
      ],
      "metadata": {
        "id": "EFKh7qSiUphe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# создаем ExhaustiveFeatureSeLlector объект.\n",
        "efs = ExhaustiveFeatureSelector(RandomForestClassifier(),\n",
        "        min_features=4,\n",
        "        max_features=8,\n",
        "        scoring='roc_auc',\n",
        "        cv=2)\n",
        "\n",
        "efs = efs.fit(X, Y)\n",
        "\n",
        "# выводим выбранные признаки\n",
        "selected_features = X_train.columns[list(efs.best_idx_)]\n",
        "print(selected_features)\n",
        "\n",
        "# выводим финальную оценку прогнозирования.\n",
        "print(efs.best_score_)"
      ],
      "metadata": {
        "id": "NtKFXN23UpDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Рекурсивное исключение признаков"
      ],
      "metadata": {
        "id": "OZ8IwgvmUyMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', random_state=42, n_jobs=-1, max_iter=50e)\n",
        "\n",
        "rfe = RFE(lr, n_features_to_select=7)\n",
        "rfe.fit(X_train, y_train)\n",
        "# X_train, y_train - входные и выходные данные с обучающей выборки соответственно.\n",
        "y_pred = rfe.predict(X_train)"
      ],
      "metadata": {
        "id": "P1i-kpDmUyl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Встроенные методы"
      ],
      "metadata": {
        "id": "em93ktioVDO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# создаем случайное дерево с вашими гипер параметрами\n",
        "model = RandomForestClassifier(n_estimators=340)\n",
        "\n",
        "# Обучаем модель на вашей выборке; Где X, y - входные и выходные данные соответственно.\n",
        "model.fit(X, y)\n",
        "\n",
        "# Подбираем самые важные признаки\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Создаем отдельный датасет для визуализации\n",
        "final_df = pd.DataFrame({\"Features\" : pd.DataFrame(X).columns, \"Importances\" : importances})\n",
        "final_df.set_index('Importances')\n",
        "\n",
        "# Сортируем их по возрастанию для лучшей визуализации\n",
        "final_df = final_df.sort_values('Importances')\n",
        "\n",
        "# Выводим на график\n",
        "final_df.plot.bar(color = 'teal')"
      ],
      "metadata": {
        "id": "oR5W9rYdVCve"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}